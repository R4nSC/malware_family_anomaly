import os
import random
import numpy as np
from torch.utils import data
from torchvision import datasets, transforms
from sklearn.model_selection import train_test_split

from config import *

class BinaryBalancedSampler:
    def __init__(self, features, labels, n_samples):
        self.features = features
        self.labels = labels

        label_counts = np.bincount(labels)
        major_label = label_counts.argmax()
        minor_label = label_counts.argmin()

        self.major_indices = np.where(labels == major_label)[0]
        self.minor_indices = np.where(labels == minor_label)[0]

        np.random.shuffle(self.major_indices)
        np.random.shuffle(self.minor_indices)

        self.used_indices = 0
        self.count = 0
        self.n_samples = n_samples
        self.batch_size = self.n_samples * 2

    def __iter__(self):
        self.count = 0
        while self.count + self.batch_size < len(self.major_indices):
            # 多数派データ(major_indices)からは順番に選び出し
            # 少数派データ(minor_indices)からはランダムに選び出す操作を繰り返す
            indices = self.major_indices[self.used_indices:self.used_indices + self.n_samples].tolist() + \
                      np.random.choice(self.minor_indices, self.n_samples, replace=False).tolist()
            yield torch.tensor(self.features[indices]), torch.tensor(self.labels[indices])

            self.used_indices += self.n_samples
            self.count += self.n_samples * 2

'''
# calculate target weights
targets_counts = np.unique(np.array(dataset.targets), return_counts=True)[1]
# weights for each sample
w = targets_counts.sum() / (targets_counts.size * targets_counts)
targets_weights = np.apply_along_axis(lambda x: w[x], 0, np.array(dataset.targets))

# split dataset and weights
indices = dict()
indices['train'], indices['tmp'] = train_test_split(np.arange(len(dataset.targets)), test_size=0.4, stratify=dataset.targets)
indices['val'], indices['test'] = train_test_split(np.array(indices['tmp']), test_size=0.5, stratify=np.array(dataset.targets)[indices['tmp']])

datasets = {x: Subset(dataset, indices=indices[x]) for x in ['train', 'val', 'test'] } 
'''


# データの前処理
def data_preprocessing(args):
    # データの前処理を設定
    data_transforms = {
        'known': transforms.Compose([
            transforms.Resize([224, 224]),
            transforms.Grayscale(num_output_channels=3),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'unknown': transforms.Compose([
            transforms.Resize([224, 224]),
            transforms.Grayscale(num_output_channels=3),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x])
                      for x in ['known', 'unknown']}

    # calculate target weights
    targets_counts = np.unique(np.array(image_datasets['known'].targets), return_counts=True)[1]
    # weights for each sample
    w = targets_counts.sum() / (targets_counts.size * targets_counts)

    targets_weights = np.apply_along_axis(lambda x: w[x], 0, np.array(image_datasets['known'].targets))

    # split dataset and weights
    indices = dict()
    indices['train'], indices['tmp'] = train_test_split(np.arange(len(image_datasets['known'].targets)), test_size=0.4,
                                                        stratify=image_datasets['known'].targets)
    indices['val'], indices['test'] = train_test_split(np.array(indices['tmp']), test_size=0.5,
                                                       stratify=np.array(image_datasets['known'].targets)[indices['tmp']])
    known_datasets = {x: data.Subset(image_datasets['known'], indices=indices[x]) for x in ['train', 'val', 'test']}

    '''
    for i in range(20):
        print('--- {}クラス ---'.format(i))
        print('train: {}, val: {}, test: {}'.format((np.array(known_datasets['train'].dataset.targets)[indices['train']] == i).sum(),
                                                    (np.array(known_datasets['val'].dataset.targets)[indices['val']] == i).sum(),
                                                    (np.array(known_datasets['test'].dataset.targets)[indices['test']] == i).sum()))
    '''

    # 訓練(train)，検証(val)，評価(test)のデータ分割サイズを定義
    train_ratio = 0.6
    val_ratio = 0.2
    train_size = int(train_ratio * len(image_datasets['known']))
    val_size = int(val_ratio * len(image_datasets['known']))
    test_size = len(image_datasets['known']) - train_size - val_size
    datasets_size = {"train": train_size, "val": val_size, "test": test_size,
                     "unknown": len(image_datasets['unknown'])}

    # 既知のマルウェアファミリのデータセットを3種類に分割
    train_datasets, val_datasets, test_datasets = torch.utils.data.random_split(image_datasets['known'],
                                                                                    [train_size, val_size, test_size])

    # データローダを作成
    train_loader = torch.utils.data.DataLoader(known_datasets['train'], batch_size=args.batch_size, shuffle=True)
    val_loader = torch.utils.data.DataLoader(known_datasets['val'], batch_size=args.batch_size, shuffle=True)
    test_loader = torch.utils.data.DataLoader(known_datasets['test'], batch_size=args.batch_size, shuffle=True)
    unknown_loader = torch.utils.data.DataLoader(image_datasets['unknown'], batch_size=args.batch_size, shuffle=True)
    data_loaders = {"train": train_loader, "val": val_loader, "test": test_loader,
                    "unknown": unknown_loader}

    # 既知クラスと未知クラスのファミリ名を保存
    class_names = {"known": image_datasets['known'].classes, "unknown": image_datasets['unknown'].classes}

    return data_loaders, datasets_size, class_names
