# This is a sample Python script.

# Press ⌃R to execute it or replace it with your code.
# Press Double ⇧ to search everywhere for classes, files, tool windows, actions, and settings.

# from PIL import Image
from io import BytesIO
import numpy as np
import os
import time
import copy
import matplotlib.pyplot as plt
import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from torch.optim import lr_scheduler
from torchvision import datasets, models, transforms


# 画像の表示
def image_show(img):
    img = img / 2 + 0.5
    np_img = img.numpy()
    plt.imshow(np.transpose(np_img, (1, 2, 0)))
    plt.show()


# マルウェアの画像化(前処理等)のデバッグ
def malware_image_debug():
    # 訓練データをランダムに取得
    data_iter = iter(data_loaders["train"])
    images, labels = data_iter.next()

    # 画像の表示
    image_show(torchvision.utils.make_grid(images))
    # ラベルの表示
    print(' '.join('%5s' % known_class_names[labels[j]] for j in range(8)))


# モデルの学習
def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    # 学習開始時間の保持
    since = time.time()

    # 初期の事前学習時点でのパラメータを保存
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    # 定めた回数分だけ学習を繰り返す
    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch+1, num_epochs))
        print('-' * 10)

        # 各エポックでTrainingとValidationを繰り返す
        for phase in ['train', 'val']:
            # 学習時と推論時で振る舞いの違うモジュールの振る舞いを変更する
            if phase == 'train':
                optimizer.step()
                scheduler.step()
                model.train(True)  # 学習モードに変更
            else:
                model.train(False)  # 推論モードに変更

            running_loss = 0.0
            running_corrects = 0

            # 用意したデータセットで学習を繰り返す
            for data in data_loaders[phase]:

                # 入力データ(バッチサイズ分)
                inputs, labels = data

                # wrap them in Variable
                if use_gpu:
                    inputs = Variable(inputs.cuda())
                    labels = Variable(labels.cuda())
                else:
                    inputs, labels = Variable(inputs), Variable(labels)

                # 勾配を０で初期化する
                optimizer.zero_grad()

                # forward
                outputs = model(inputs)
                _, preds = torch.max(outputs.data, 1)
                loss = criterion(outputs, labels)

                # 学習フェーズの時のみ，誤差逆伝播とパラメータ更新を行う
                if phase == 'train':
                    loss.backward()  # 誤差逆伝播にて勾配を求める
                    optimizer.step()  # optimizerでパラメータを更新

                # statistics
                running_loss += loss.data.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / datasets_size[phase]
            epoch_acc = running_corrects / datasets_size[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))

            # deep copy the model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model


if __name__ == '__main__':
    # データの前処理
    data_transforms = {
        'known': transforms.Compose([
            transforms.Resize([224, 224]),
            transforms.Grayscale(num_output_channels=3),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'unknown': transforms.Compose([
            transforms.Resize([224, 224]),
            transforms.Grayscale(num_output_channels=3),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    # MalimgデータセットのURL
    data_dir = './resources/malimg_paper_dataset_imgs/'
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])
                      for x in ['known', 'unknown']}

    # 訓練(train)，検証(val)，評価(test)のデータ分割サイズを定義
    train_ratio = 0.6
    val_ratio = 0.2
    train_size = int(train_ratio * len(image_datasets['known']))
    val_size = int(val_ratio * len(image_datasets['known']))
    test_size = len(image_datasets['known']) - train_size - val_size
    datasets_size = {"train": train_size, "val": val_size, "test": test_size,
                     "unknown": len(image_datasets['unknown'])}

    # 既知のマルウェアファミリのデータセットを3種類に分割
    train_datasets, val_datasets, test_datasets = torch.utils.data.random_split(image_datasets['known'],
                                                                                    [train_size, val_size, test_size])
    # データローダを作成
    batch_size = 64
    train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_datasets, batch_size=batch_size, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=batch_size, shuffle=True)
    unknown_loader = torch.utils.data.DataLoader(image_datasets['unknown'], batch_size=batch_size, shuffle=True)
    data_loaders = {"train": train_loader, "val": val_loader, "test": test_loader,
                    "unknown": unknown_loader}

    # 既知クラスと未知クラスのファミリ名を保存
    known_class_names = image_datasets['known'].classes
    unknown_class_names = image_datasets['unknown'].classes

    # GPUを学習に利用可能かどうか
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        print('Use GPU')
    else:
        print('Do not use GPU')

    # AlexNetの学習済みモデルをロード
    alex_net = models.alexnet(pretrained=True)

    # AlexNetの最終層の次元数を変更
    alx_lt = list(alex_net.classifier)
    alx_lt[6] = nn.Linear(in_features=4096, out_features=20, bias=True)
    alex_net.classifier = nn.Sequential(alx_lt[0], alx_lt[1], alx_lt[2], alx_lt[3], alx_lt[4], alx_lt[5], alx_lt[6])

    if use_gpu:
        alex_net = alex_net.cuda()

    criterion_ft = nn.CrossEntropyLoss()

    # Observe that all parameters are being optimized
    optimizer_ft = optim.SGD(alex_net.parameters(), lr=0.001, momentum=0.9)

    # Decay LR by a factor of 0.1 every 7 epochs
    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

    model_ft = alex_net
    # model_ft = train_model(alex_net, criterion_ft, optimizer_ft, exp_lr_scheduler, num_epochs=1)

    # テストフェーズ
    print('test_phase')
    correct = 0
    total = 0
    with torch.no_grad():
        for data in data_loaders['test']:
            images, labels = data

            if use_gpu:
                images = Variable(images.cuda())
                labels = Variable(labels.cuda())
            else:
                images, labels = Variable(images), Variable(labels)

            outputs = model_ft(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the {:d} test images: {:.4f}'.format(datasets_size['test'], correct / total))
